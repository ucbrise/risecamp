{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 6 - Training with Ray and Serving with Clipper\n",
    "\n",
    "**GOAL:** The goal of this exercise is to show how to train a policy with Ray and to deploy it with Clipper in a fun, interactive way.\n",
    "\n",
    "We will train an agent to play Pong, and then we will play Pong against the policy that we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import pong_py\n",
    "import ray\n",
    "import cloudpickle\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents import ppo\n",
    "import flor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.setNotebookName('flor_rl_exercise04.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = flor.Experiment('integration_imitation_risecamp2018').__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_data = ex.artifact('imitation_data.csv', 'imitation_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def preproc_imitation(imitation_data, procd_imitation_data, **kwargs):\n",
    "    import pandas as pd\n",
    "    df_data = pd.read_csv(imitation_data)\n",
    "    df_data.columns = [\"label\",\"paddle_y\",\"ball_x\",\"ball_y\",\"ball_dx\",\"ball_dy\",\"x_prev\",\"y_prev\"]\n",
    "\n",
    "    def convert_label(label):\n",
    "        \"\"\"Convert labels into numeric values\"\"\"\n",
    "        if(label==\"down\"):\n",
    "            return 1\n",
    "        elif(label==\"up\"):\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    df_data['label'] = df_data['label'].apply(convert_label)\n",
    "    df_data.loc[:, \"paddle_y\":\"y_prev\"] = df_data.loc[:, \"paddle_y\":\"y_prev\"]/500.0\n",
    "    df_data.to_json(procd_imitation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_preproc_imitation = ex.action(preproc_imitation, [imitation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procd_imitation_data = ex.artifact('imitation_data.json', 'procd_imitation_data', do_preproc_imitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def train_imitation_model(procd_imitation_data, imitation_model, **kwargs):\n",
    "    import cloudpickle\n",
    "    import pandas as pd\n",
    "    from sklearn import linear_model\n",
    "    df_data = pd.read_json(procd_imitation_data)\n",
    "    \n",
    "    labels = df_data['label']\n",
    "    training_data= df_data.drop(['label'], axis=1)\n",
    "\n",
    "    model = linear_model.LogisticRegression()\n",
    "    model.fit(training_data, labels)\n",
    "    with open(imitation_model, 'wb') as f:\n",
    "        cloudpickle.dump(model, f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train_imitation_model = ex.action(train_imitation_model, [procd_imitation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_model = ex.artifact('model.pkl', 'imitation_model', do_train_imitation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.__exit__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_model.pull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = flor.Experiment('integration_rl_risecamp2018').__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def start_ray(**kwargs):\n",
    "    try:\n",
    "        ray.get([])\n",
    "    except:\n",
    "        ray.init()    \n",
    "    return {'exit_code': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an agent that can be trained using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the `PPOAgent` for some number of iterations.\n",
    "\n",
    "**EXERCISE:** You will need to experiment with the number of iterations as well as with the configuration to get the agent to learn something reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint the agent so that the relevant model can be saved and deployed to Clipper. We save the name of the checkpoint file in `metadata.json` so the model container knows how to restore the policy checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flor.func\n",
    "def train_agent(env_config, num_iterations, **kwargs):\n",
    "    register_env(\"my_env\", lambda ec: pong_py.PongJSEnv())\n",
    "    agent = ppo.PPOAgent(env=\"my_env\", config={\"env_config\": {}})\n",
    "    for i in range(num_iterations):\n",
    "        result = agent.train()\n",
    "    checkpoint_path = agent.save()\n",
    "    return {'checkpoint_path': checkpoint_path}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_start_ray = ex.action(start_ray, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_code = ex.literal(name='exit_code', parent=do_start_ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = ex.literal({}, 'env_config') # TODO: Fill env_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = ex.literal(2, 'num_iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train_agent = ex.action(train_agent, [env_config, num_iterations, exit_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ex.literal(name='checkpoint_path', parent=do_train_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.__exit__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: source Pages: 1 -->\n",
       "<svg width=\"375pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 375.00 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<title>source</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-328 371,-328 371,4 -4,4\"/>\n",
       "<!-- 2 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"158,-324 0,-324 0,-288 158,-288 158,-324\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">flor_rl_exercise04.ipynb</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"158\" cy=\"-90\" rx=\"51.9908\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"158\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">train_agent</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M71.1529,-287.9132C58.7145,-256.2292 38.5445,-189.7163 66,-144 76.0566,-127.2546 93.4424,-115.1414 110.4112,-106.6858\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.1457,-109.7396 119.7463,-102.3584 109.2017,-103.3888 112.1457,-109.7396\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"114\" cy=\"-234\" rx=\"42.4939\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"114\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">start_ray</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M87.832,-287.8314C91.7581,-279.7547 96.463,-270.0761 100.7939,-261.1668\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.0679,-262.4373 105.2921,-251.9134 97.7723,-259.3769 104.0679,-262.4373\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"247,-180 165,-180 165,-144 247,-144 247,-180\"/>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"165,-144 247,-144 \"/>\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">env_config</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M193.8876,-143.8314C188.3313,-135.497 181.6378,-125.4567 175.5441,-116.3162\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"178.4015,-114.2925 169.9423,-107.9134 172.5771,-118.1754 178.4015,-114.2925\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"367,-180 265,-180 265,-144 367,-144 367,-180\"/>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"265,-144 367,-144 \"/>\n",
       "<text text-anchor=\"middle\" x=\"316\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">num_iterations</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M276.1299,-143.8314C252.4833,-133.0557 222.579,-119.4284 198.7752,-108.5811\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.1983,-105.3834 189.6472,-104.4215 197.2956,-111.7532 200.1983,-105.3834\"/>\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"214,-36 102,-36 102,0 214,0 214,-36\"/>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"102,0 214,0 \"/>\n",
       "<text text-anchor=\"middle\" x=\"158\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">checkpoint_path</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;0 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M158,-71.8314C158,-64.131 158,-54.9743 158,-46.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.5001,-46.4132 158,-36.4133 154.5001,-46.4133 161.5001,-46.4132\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" stroke=\"transparent\" points=\"147,-180 75,-180 75,-144 147,-144 147,-180\"/>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"75,-144 147,-144 \"/>\n",
       "<text text-anchor=\"middle\" x=\"111\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">exit_code</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M113.243,-215.8314C112.9221,-208.131 112.5406,-198.9743 112.184,-190.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"115.6806,-190.2589 111.7672,-180.4133 108.6867,-190.5503 115.6806,-190.2589\"/>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;1 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M122.8601,-143.8314C128.3006,-135.497 134.8547,-125.4567 140.8214,-116.3162\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"143.7711,-118.2004 146.3065,-107.9134 137.9094,-114.374 143.7711,-118.2004\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7fcdd1abee10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/raylogs/.\n",
      "Waiting for redis server at 127.0.0.1:43702 to respond...\n",
      "Waiting for redis server at 127.0.0.1:25769 to respond...\n",
      "Starting local scheduler with the following resources: {'CPU': 2, 'GPU': 0}.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui62783.ipynb?token=49fd9977be77088280ee460447d8eb3fbab09cc2c8a90a42\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jovyan/ray_results/2018-09-20_20-33-19p21vx970 -> None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "== sgd epochs ==\n",
      "0 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1237.3672, 'policy_loss': 0.00051114941, 'vf_loss': 1237.3665, 'vf_explained_var': 0.00022488087, 'kl': 1.9954146e-05, 'entropy': 1.0985816}\n",
      "1 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1236.7732, 'policy_loss': 0.00021207938, 'vf_loss': 1236.7729, 'vf_explained_var': 0.00025888346, 'kl': 5.3080501e-05, 'entropy': 1.0985498}\n",
      "2 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1235.7518, 'policy_loss': 0.00010186294, 'vf_loss': 1235.7517, 'vf_explained_var': 0.00047820434, 'kl': 0.00025390455, 'entropy': 1.0983365}\n",
      "3 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1234.3628, 'policy_loss': -0.00033348426, 'vf_loss': 1234.363, 'vf_explained_var': 0.00081955269, 'kl': 0.00039894241, 'entropy': 1.0981958}\n",
      "4 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1233.5073, 'policy_loss': -0.00056894496, 'vf_loss': 1233.5078, 'vf_explained_var': 5.48549e-05, 'kl': 0.00051717041, 'entropy': 1.0980686}\n",
      "5 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1232.8588, 'policy_loss': -0.000721741, 'vf_loss': 1232.8594, 'vf_explained_var': -0.00034198165, 'kl': 0.00097186479, 'entropy': 1.097598}\n",
      "6 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1232.166, 'policy_loss': -0.00093691237, 'vf_loss': 1232.1667, 'vf_explained_var': -0.0011871774, 'kl': 0.0010145612, 'entropy': 1.0975659}\n",
      "7 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1231.3252, 'policy_loss': -0.0012456011, 'vf_loss': 1231.3262, 'vf_explained_var': -0.0014334079, 'kl': 0.0012442142, 'entropy': 1.0973345}\n",
      "8 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1230.3708, 'policy_loss': -0.0013915887, 'vf_loss': 1230.3721, 'vf_explained_var': -0.0019582268, 'kl': 0.0014635129, 'entropy': 1.0971084}\n",
      "9 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1229.2651, 'policy_loss': -0.0016632313, 'vf_loss': 1229.2664, 'vf_explained_var': -0.0021016914, 'kl': 0.0022890319, 'entropy': 1.0962809}\n",
      "10 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1228.1594, 'policy_loss': -0.0018012845, 'vf_loss': 1228.1606, 'vf_explained_var': -0.0022874512, 'kl': 0.0024401848, 'entropy': 1.0961242}\n",
      "11 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1226.9424, 'policy_loss': -0.002171993, 'vf_loss': 1226.9441, 'vf_explained_var': -0.00088405423, 'kl': 0.0027096109, 'entropy': 1.0958502}\n",
      "12 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1225.6597, 'policy_loss': -0.0024437939, 'vf_loss': 1225.6615, 'vf_explained_var': 0.00012962148, 'kl': 0.0032029711, 'entropy': 1.0953417}\n",
      "13 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1224.4662, 'policy_loss': -0.0026220609, 'vf_loss': 1224.468, 'vf_explained_var': 0.00080381893, 'kl': 0.0034052534, 'entropy': 1.0951409}\n",
      "14 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1223.5066, 'policy_loss': -0.0028608995, 'vf_loss': 1223.5088, 'vf_explained_var': 0.0016790088, 'kl': 0.0037709048, 'entropy': 1.0947688}\n",
      "15 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1222.8502, 'policy_loss': -0.0031197791, 'vf_loss': 1222.8524, 'vf_explained_var': 0.0022060312, 'kl': 0.0042400286, 'entropy': 1.0942812}\n",
      "16 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1222.3525, 'policy_loss': -0.0034271078, 'vf_loss': 1222.355, 'vf_explained_var': 0.0026507769, 'kl': 0.0051701488, 'entropy': 1.0933502}\n",
      "17 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1222.022, 'policy_loss': -0.0035889745, 'vf_loss': 1222.0244, 'vf_explained_var': 0.0030543208, 'kl': 0.0051328866, 'entropy': 1.0933874}\n",
      "18 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.7997, 'policy_loss': -0.0038166284, 'vf_loss': 1221.8025, 'vf_explained_var': 0.0034885462, 'kl': 0.0054193651, 'entropy': 1.0931051}\n",
      "19 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.6482, 'policy_loss': -0.0037617898, 'vf_loss': 1221.651, 'vf_explained_var': 0.0031779185, 'kl': 0.0051176785, 'entropy': 1.0933914}\n",
      "20 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.5349, 'policy_loss': -0.0039477292, 'vf_loss': 1221.5378, 'vf_explained_var': 0.0032871347, 'kl': 0.0053039314, 'entropy': 1.093209}\n",
      "21 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.4565, 'policy_loss': -0.0041116737, 'vf_loss': 1221.4597, 'vf_explained_var': 0.0032104962, 'kl': 0.0050468165, 'entropy': 1.0934746}\n",
      "22 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.3778, 'policy_loss': -0.0042539444, 'vf_loss': 1221.3809, 'vf_explained_var': 0.0028008074, 'kl': 0.0055169053, 'entropy': 1.0929922}\n",
      "23 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.3347, 'policy_loss': -0.0043017622, 'vf_loss': 1221.3381, 'vf_explained_var': 0.0026696753, 'kl': 0.004924309, 'entropy': 1.0935886}\n",
      "24 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.2905, 'policy_loss': -0.0043492904, 'vf_loss': 1221.2937, 'vf_explained_var': 0.0022441316, 'kl': 0.0051076128, 'entropy': 1.093406}\n",
      "25 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.262, 'policy_loss': -0.0048088357, 'vf_loss': 1221.2657, 'vf_explained_var': 0.0024510659, 'kl': 0.0057570431, 'entropy': 1.0927316}\n",
      "26 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.2371, 'policy_loss': -0.0049742162, 'vf_loss': 1221.2411, 'vf_explained_var': 0.0021801963, 'kl': 0.005791808, 'entropy': 1.0927103}\n",
      "27 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.2209, 'policy_loss': -0.005092158, 'vf_loss': 1221.2249, 'vf_explained_var': 0.0021091085, 'kl': 0.0055172006, 'entropy': 1.0929799}\n",
      "28 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.2041, 'policy_loss': -0.0053492738, 'vf_loss': 1221.2083, 'vf_explained_var': 0.0022467747, 'kl': 0.0055084731, 'entropy': 1.0929966}\n",
      "29 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1221.1947, 'policy_loss': -0.0054955985, 'vf_loss': 1221.1991, 'vf_explained_var': 0.0022748765, 'kl': 0.0057839388, 'entropy': 1.0927122}\n",
      "== sgd epochs ==\n",
      "0 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1829.5243, 'policy_loss': -0.00019758099, 'vf_loss': 1829.5245, 'vf_explained_var': 0.0083685322, 'kl': 7.4707561e-05, 'entropy': 1.0916959}\n",
      "1 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1825.6112, 'policy_loss': -0.00086943869, 'vf_loss': 1825.6121, 'vf_explained_var': 0.0086327503, 'kl': 0.00026225441, 'entropy': 1.0919987}\n",
      "2 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1824.8158, 'policy_loss': -0.001370006, 'vf_loss': 1824.817, 'vf_explained_var': 0.0077697309, 'kl': 0.00071150478, 'entropy': 1.0919191}\n",
      "3 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1824.0134, 'policy_loss': -0.0018046027, 'vf_loss': 1824.0148, 'vf_explained_var': 0.0061652758, 'kl': 0.0012042805, 'entropy': 1.0918125}\n",
      "4 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1823.2887, 'policy_loss': -0.0022918349, 'vf_loss': 1823.2904, 'vf_explained_var': 0.004436316, 'kl': 0.0014546558, 'entropy': 1.0914005}\n",
      "5 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.9194, 'policy_loss': -0.0028876355, 'vf_loss': 1822.922, 'vf_explained_var': 0.0033261257, 'kl': 0.0021166776, 'entropy': 1.0909828}\n",
      "6 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.8475, 'policy_loss': -0.0030638897, 'vf_loss': 1822.8501, 'vf_explained_var': 0.0030781743, 'kl': 0.0028776436, 'entropy': 1.0906777}\n",
      "7 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.8218, 'policy_loss': -0.0037088098, 'vf_loss': 1822.8248, 'vf_explained_var': 0.0031923095, 'kl': 0.0038188996, 'entropy': 1.089745}\n",
      "8 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.8115, 'policy_loss': -0.0041793878, 'vf_loss': 1822.8147, 'vf_explained_var': 0.0028729015, 'kl': 0.0041588969, 'entropy': 1.089415}\n",
      "9 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.8082, 'policy_loss': -0.0045330375, 'vf_loss': 1822.8116, 'vf_explained_var': 0.0029341348, 'kl': 0.0048076217, 'entropy': 1.0885179}\n",
      "10 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.8048, 'policy_loss': -0.0047950861, 'vf_loss': 1822.8083, 'vf_explained_var': 0.0026887925, 'kl': 0.0053195353, 'entropy': 1.0878469}\n",
      "11 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.8026, 'policy_loss': -0.0051489417, 'vf_loss': 1822.8064, 'vf_explained_var': 0.0026087721, 'kl': 0.0061054276, 'entropy': 1.0873549}\n",
      "12 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.8011, 'policy_loss': -0.0053079966, 'vf_loss': 1822.8051, 'vf_explained_var': 0.0026231408, 'kl': 0.0061194473, 'entropy': 1.0871824}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.8003, 'policy_loss': -0.0054769488, 'vf_loss': 1822.8042, 'vf_explained_var': 0.002406972, 'kl': 0.0067007658, 'entropy': 1.0863574}\n",
      "14 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7998, 'policy_loss': -0.0057910811, 'vf_loss': 1822.804, 'vf_explained_var': 0.0024702742, 'kl': 0.0074436907, 'entropy': 1.0860183}\n",
      "15 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7991, 'policy_loss': -0.0059746895, 'vf_loss': 1822.8033, 'vf_explained_var': 0.002432229, 'kl': 0.0073204646, 'entropy': 1.0861238}\n",
      "16 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7985, 'policy_loss': -0.005921213, 'vf_loss': 1822.8031, 'vf_explained_var': 0.0023760276, 'kl': 0.0073021781, 'entropy': 1.0855323}\n",
      "17 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7983, 'policy_loss': -0.0063434225, 'vf_loss': 1822.803, 'vf_explained_var': 0.002287311, 'kl': 0.0077751838, 'entropy': 1.0848862}\n",
      "18 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7981, 'policy_loss': -0.006511474, 'vf_loss': 1822.8032, 'vf_explained_var': 0.0027005789, 'kl': 0.0081834542, 'entropy': 1.0845008}\n",
      "19 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7974, 'policy_loss': -0.0066646077, 'vf_loss': 1822.8024, 'vf_explained_var': 0.0023855607, 'kl': 0.0082969274, 'entropy': 1.0843124}\n",
      "20 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7971, 'policy_loss': -0.0066894949, 'vf_loss': 1822.8022, 'vf_explained_var': 0.0023776831, 'kl': 0.0087537114, 'entropy': 1.0835673}\n",
      "21 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7968, 'policy_loss': -0.0070269443, 'vf_loss': 1822.8021, 'vf_explained_var': 0.0022329977, 'kl': 0.0087301042, 'entropy': 1.0835288}\n",
      "22 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7964, 'policy_loss': -0.0072240243, 'vf_loss': 1822.8019, 'vf_explained_var': 0.0022194481, 'kl': 0.0088784778, 'entropy': 1.0837431}\n",
      "23 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7965, 'policy_loss': -0.007399037, 'vf_loss': 1822.8018, 'vf_explained_var': 0.0021230597, 'kl': 0.0093984967, 'entropy': 1.083162}\n",
      "24 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7964, 'policy_loss': -0.007531873, 'vf_loss': 1822.8018, 'vf_explained_var': 0.0022183331, 'kl': 0.0092105148, 'entropy': 1.0832015}\n",
      "25 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7958, 'policy_loss': -0.0076472312, 'vf_loss': 1822.8016, 'vf_explained_var': 0.0022504984, 'kl': 0.0093824668, 'entropy': 1.0827314}\n",
      "26 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7957, 'policy_loss': -0.00761138, 'vf_loss': 1822.8013, 'vf_explained_var': 0.0020246257, 'kl': 0.00905935, 'entropy': 1.0830721}\n",
      "27 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7957, 'policy_loss': -0.0076647229, 'vf_loss': 1822.8014, 'vf_explained_var': 0.0020720074, 'kl': 0.0090975184, 'entropy': 1.0831829}\n",
      "28 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7952, 'policy_loss': -0.0081752883, 'vf_loss': 1822.8011, 'vf_explained_var': 0.0020968874, 'kl': 0.0098867528, 'entropy': 1.082505}\n",
      "29 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 1822.7954, 'policy_loss': -0.0080235125, 'vf_loss': 1822.8009, 'vf_explained_var': 0.0020629438, 'kl': 0.0095175272, 'entropy': 1.0823653}\n"
     ]
    }
   ],
   "source": [
    "checkpoint.pull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Against the Policy\n",
    "\n",
    "In this section, we will play Pong against the policy that we just trained. The game will be played in your browser, and the policy that we trained will be served by Clipper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Deploy your policy using Clipper. Follow the instructions that get printed below to play Pong against the deployed policy. You'll need to deploy all of the data that is saved in the directory `os.path.dirname(checkpoint_path)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the `clipper_admin` library and use that to create a new Clipper instance to serve the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create your ClipperConnection, you need to tell it how to communicate with the Docker service and Clipper. You can use the following command to get the Docker IP address. Use that address when you create your `ClipperConnection` in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make logging work correctly in the Jupyter notebook\n",
    "import logging\n",
    "import sys\n",
    "import subprocess32 as subprocess\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from clipper_admin import DockerContainerManager, ClipperConnection\n",
    "docker_ip = subprocess.check_output(\"./get_docker_ip.sh\").strip()\n",
    "clipper_conn = ClipperConnection(DockerContainerManager(docker_ip_address=docker_ip))\n",
    "# Add a call to stop all in case you still have Clipper running from the earlier exercises\n",
    "clipper_conn.stop_all()\n",
    "clipper_conn.start_clipper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, deploy the saved policy checkpoint to Clipper using a Docker image we created for this exercise (similar to the TensorFlow model container in the Clipper tutorial). If you're curious, you can find the custom model container code on [GitHub](https://github.com/ucbrise/risecamp/blob/077aa51078e2043d4d3d2d539e256c30c259678e/rl_and_pong/pong_model_container.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_name = \"pong-policy\"\n",
    "app_name = \"pong\"\n",
    "clipper_conn.build_and_deploy_model(\n",
    "    name=model_name,\n",
    "    version=1,\n",
    "    input_type=\"doubles\",\n",
    "    model_data_path=os.path.dirname(checkpoint_path),\n",
    "    base_image=\"clipper/risecamp-pong-container\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, register a Clipper application and link it the deployed policy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"pong\"\n",
    "clipper_conn.register_application(name=app_name, default_output=\"0\", input_type=\"doubles\", slo_micros=100000)\n",
    "clipper_conn.link_model_to_app(app_name=app_name, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have deployed your policy to Clipper, you will start a Pong application that will let you play against your policy in the browser.\n",
    "\n",
    "When you start the application, you need to tell it where Clipper is running in order for the Pong application to request predictions from Clipper. `ClipperConnection` provides the `get_query_addr()` method to get the IP address and port on which Clipper is listening for incoming prediction requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_addr = clipper_conn.get_query_addr()\n",
    "print(\"Clipper address: {}\".format(clipper_addr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start the Pong webserver. It will print out the URL it's running on after it starts. Copy and paste that URL into your browser and press \"1\" to play against your policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess32 as subprocess\n",
    "server_handle = subprocess.Popen([\"./start_webserver.sh\", clipper_addr], stdout=subprocess.PIPE)\n",
    "print(server_handle.stdout.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a New Policy\n",
    "\n",
    "The first policy that you deploy probably won't be a very strong competitor, especially if you only trained it for a few iterations. Try training it for more iterations and deploying the new policy to Clipper. Clipper will automatically switch the Pong application to query the new version of the policy. You don't need to reload the page or even restart the game.\n",
    "\n",
    "For your convenience, we've copied the relevant cells from above to train the policy for more iterations and deploy it Clipper. You can run this cell as many times as you want. Don't forget to increment the version number of the model each time you deploy to Clipper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for more iterations\n",
    "for i in range(50):\n",
    "    result = agent.train()\n",
    "    \n",
    "# Save the new policy\n",
    "checkpoint_path = agent.save()\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_file = os.path.basename(checkpoint_path)\n",
    "with open(os.path.join(checkpoint_dir, \"metadata.json\"), \"w\") as f:\n",
    "    json.dump({\"checkpoint\": checkpoint_file}, f)\n",
    "    \n",
    "# Deploy the new policy to Clipper.\n",
    "clipper_conn.build_and_deploy_model(\n",
    "    name=model_name,\n",
    "    version=2, # If you run this more than once, don't forget to keep updating the version.\n",
    "    input_type=\"doubles\",\n",
    "    model_data_path=os.path.dirname(checkpoint_path),\n",
    "    base_image=\"clipper/risecamp-pong-container\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyWren RISECamp, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics with PyWren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use PyWren explore a dataset of Wikipedia statistics.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "We've prepared an S3 bucket with 20GB of Wikipedia traffic data obtained from http://aws.amazon.com/datasets/4182. To make the analysis more feasible for the short time you're here, we've shortened the dataset to three days worth of data (May 5 to May 7, 2009; roughly 20G and 329 million entries). \n",
    "\n",
    "Let's take a look into the bucket with our dataset. We'll print a few files from a few files from our bucket.\n",
    "\n",
    "***Execute*** the code below to print out the names of the first 20 files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from training import wikipedia_bucket, list_keys_with_prefix, read_from_s3\n",
    "\n",
    "filenames = list_keys_with_prefix(wikipedia_bucket, \"wikistats_20090505_restricted-01/\")\n",
    "for filename in filenames[:20]:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 74 files (2 of which are intentionally left empty). Each file consists of a list of records. Let's go take a look into the first file. This may take a while.\n",
    "\n",
    "***Execute*** the code below to print out 20 records from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read a file into memory, and split on newlines.\n",
    "records = read_from_s3(wikipedia_bucket, \"wikistats_20090505_restricted-01/part-00001\").split(\"\\n\")\n",
    "\n",
    "print(\"The total number of records in this file is {}, but here are the first 20\".format(len(records)))\n",
    "for i in range(20):\n",
    "    print(records[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record some stats for a single Wikipedia page in a given time frame. The schema is:\n",
    "\n",
    "`<date_time> <project_code> <page_name> <page_views> <page_size>`\n",
    "\n",
    "- `<date_time>` specifies a date in YYYYMMDD-HHmmSS format (year, month, day, hour minute, second). This is the starting date-time of the record.\n",
    "- `<project_code>` specifies the language the page is written in.\n",
    "- `<page_title>` gives the page title.\n",
    "- `<page_views>` gives the number of page views in the hour-long time slot starting at `<data_time>`. \n",
    "- `<page_size>` gives the size in bytes of the  page.\n",
    "\n",
    "Now that we have a better understanding of the structure of our data, we can start running some interesting queries. \n",
    "\n",
    "Because the data are so large, it takes us quite a while to load just one file, as we saw in the previous exercise. You could imagine it would take way too long to process all of the data sequentially. We can use [PyWren](https://www.youtube.com/watch?v=VSaDPc1Cs5U) to help with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### To the ~~Batmobile~~ Cloud!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PyWren, we can invoke lambdas to do everything in parallel in the cloud, including reading and writing files from S3. First we'll again set up an executor.\n",
    "\n",
    "***Execute*** the code below to initialize an executor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywren\n",
    "pwex = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Count\n",
    "Let’s see how many records in total are in this data set.\n",
    "\n",
    "***Exercise***: modify `count` to return the number of records in a given file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    #FIX ME\n",
    "    return 0\n",
    "\n",
    "print (\"invoking pywren jobs...\")\n",
    "futures = pwex.map(count, filenames)\n",
    "print (\"working...\")\n",
    "pywren.wait(futures)\n",
    "\n",
    "result = sum([f.result() for f in futures])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should launch 73 PyWren tasks. After finishing the job, let's plot again to check the execution. This should look more interesting than the simple job before.\n",
    "\n",
    "***Execute*** the code below to show a plot of the executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from training import plot_pywren_execution\n",
    "plot_pywren_execution(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visits for English Pages\n",
    "Recall from above that the second field of a record is the `project code` For example, the project code `en` indicates an English page. Let’s calculate the total view count on all English pages for each date in our dataset.\n",
    "\n",
    "***Exercise:*** modify `is_page_english` so that it return true if a given record corresponds to an English page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from functools import reduce\n",
    "\n",
    "def aggregate_count(key_value_list):\n",
    "    \n",
    "    def reduce_f(obj1, obj2):\n",
    "        return(obj1[0], obj1[1] + obj2[1])\n",
    "    \n",
    "    counts = [reduce(reduce_f, group) for _, group \n",
    "          in groupby(sorted(key_value_list), key=itemgetter(0))]\n",
    "    return counts\n",
    "\n",
    "def english_page_count(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    \n",
    "    # return True if a record corresponds to an english page and false otherwise.\n",
    "    def is_page_english(page):\n",
    "        #FIX ME\n",
    "        return False\n",
    "    \n",
    "    # filter out the english pages\n",
    "    en_pages = filter(is_page_english, data.split(\"\\n\"))\n",
    "    \n",
    "    # projection to create (date, pagecount) pairs\n",
    "    def make_date_viewcount_pair(record):\n",
    "        split_record = record.split()\n",
    "        \n",
    "        # the daate is the first entry of the record.\n",
    "        # We only want the YYYYMMDD characters.\n",
    "        date = split_record[0][:8]\n",
    "        \n",
    "        view_count = int(split_record[3])\n",
    "        return (date, view_count)\n",
    "    \n",
    "    en_kvpair_list = [make_date_viewcount_pair(p) for p in en_pages]\n",
    "\n",
    "    return aggregate_count(en_kvpair_list)\n",
    "    \n",
    "futures = pwex.map(english_page_count, filenames)\n",
    "pywren.wait(futures)\n",
    "\n",
    "results = [f.result() for f in futures]\n",
    "en_page_counts_by_date = aggregate_count([x for y in results for x in y])\n",
    "print(en_page_counts_by_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bonus: Finding Trends for the Time of Day.\n",
    "One cool thing we can do with our data-set is to look at how popular pages trend with the time of day.\n",
    "\n",
    "Suppose we wanted to view the pages that got the most views from 5PM-6PM each day. One \"naive\" solution would be to `map` a function that takes in a filename and returns a list of `(page_views, title)` pairs for each record in the given time range, and sort the aggregated results locally. This approach however, involves us iterating through some `O(million)` records on our local machine, when we ideally could do some more work in parallel.\n",
    "\n",
    "To offload more work to the cloud, we could change our `map` function to return the 15 most popular records in each file. This would reduce the amount of work we do on our single node.\n",
    "\n",
    "***Exercise*** Fix `fifteen_largest` and `process_file` so that it behaves as intended. Try running queries on different times of day, and note any interesting findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywren\n",
    "pwex = pywren.default_executor()\n",
    "\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "# Takes a list of (page_view, page_title) pairs\n",
    "# and returns the 15 pages with the largest page_view.\n",
    "def fifteen_largest(list_of_pairs):\n",
    "    # Hint. You can use a min heap.\n",
    "    # https://docs.python.org/2/library/heapq.html\n",
    "    \n",
    "    # FIX ME.\n",
    "    h = []\n",
    "    return h\n",
    "\n",
    "\n",
    "def process_file(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    data = data.split(\"\\n\")\n",
    "\n",
    "    data = filter(page_filter, data)\n",
    "    # Filter out pages that are not in English, or are \n",
    "    # administrative/error pages.\n",
    "    def page_filter(record):\n",
    "        arr = record.split(\" \")\n",
    "        # filter out any empty lines.\n",
    "        if len(arr) < 5:\n",
    "            return False\n",
    "        if arr[1] == \"en\":\n",
    "            # Filter out any error or special pages.\n",
    "            if \"404_error\" not in arr[2] and \"Main\" not in arr[2] and \"Special:\" not in arr[2]:\n",
    "                # Only approve records starting at 17:00:00\n",
    "                        \n",
    "                #FIX ME\n",
    "                if arr[0][-6:] == \"\":\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    # Given a record, return a tuple of \n",
    "    # (page_views, page_title).\n",
    "    # Remember that page_view should be an int.\n",
    "    def extract_kv_pair(record):\n",
    "        record = record.split(\" \")\n",
    "        \n",
    "        #FIX ME\n",
    "        return (None, None)\n",
    "    data = map(extract_kv_pair, data)\n",
    "    return fifteen_largest(data)\n",
    "\n",
    "\n",
    "futures = pwex.map(process_file, filenames)\n",
    "results = pywren.get_all_results(futures)\n",
    "\n",
    "flattened_list = []\n",
    "for result in results:\n",
    "    for elem in result:\n",
    "        flattened_list.append(elem)\n",
    "\n",
    "for item in fifteen_largest(flattened_list):\n",
    "    print(item)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyWren RISECamp, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics with PyWren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use PyWren explore a dataset of Wikipedia records.\n",
    "\n",
    "## 0. The Data\n",
    "\n",
    "We've prepared an S3 bucket with 20GB of Wikipedia traffic statistics data obtained from http://aws.amazon.com/datasets/4182. To make the analysis more feasible for the short time you're here, we've shortened the dataset to three days worth of data (May 5 to May 7, 2009; roughly 20G and 329 million entries). \n",
    "\n",
    "Let's take a look into the bucket with our dataset. We'll print a few files from a few files from our bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from training import wikipedia_bucket, list_keys_with_prefix, read_from_s3\n",
    "\n",
    "filenames = list_keys_with_prefix(wikipedia_bucket, \"wikistats_20090505_restricted-01/\")\n",
    "for filename in filenames[:20]:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 74 files (2 of which are intentionally left empty). Each file consists of a list of records. Let's go take a look into the first file. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read a file into memory, and split on newlines.\n",
    "records = read_from_s3(wikipedia_bucket, \"wikistats_20090505_restricted-01/part-00001\").split(\"\\n\")\n",
    "\n",
    "print(\"The total number of records in this file is {}, but here are the first 20\".format(len(records)))\n",
    "for i in range(20):\n",
    "    print(records[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record has stats for a single Wikipedia page. The schema is:\n",
    "\n",
    "`<date_time> <project_code> <page_name> <page_views> <page_size>`\n",
    "\n",
    "- `<date_time>` specifies a date in YYYYMMDD-HHmmSS format (year, month, day, hour minute, second).\n",
    "- `<project_code>` specifies the language the page is written in.\n",
    "- `<page_title>` gives the page title.\n",
    "- `<page_views>` gives the number of page views in the hour-long time slot starting at `<data_time>`. \n",
    "- `<page_size>` gives the size in bytes of the  page.\n",
    "\n",
    "Now that we have a better understanding of the structure of our data, we can start running some interesting queries. \n",
    "\n",
    "Because the data are so large, it takes us quite a while to load just one file, as we saw in the previous exercise. You could imagine it would take way too long to process all of the data sequentially. Thankfully, we have [PyWren](https://www.youtube.com/watch?v=VSaDPc1Cs5U)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. To the ~~Batmobile~~ Cloud!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PyWren, we can invoke lambdas to do everything in parallel in the cloud, including reading and writing files from S3. First we'll again set up an executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywren\n",
    "pwex = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Count\n",
    "Let’s see how many records in total are in this data set.\n",
    "\n",
    "***Exercise:*** modify the code below to count the number of records in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = list_keys_with_prefix(wikipedia_bucket, \"wikistats_20090505_restricted-01/\")\n",
    "\n",
    "def count(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    # fill me in:\n",
    "    return \n",
    "\n",
    "print (\"invoking pywren jobs...\")\n",
    "futures = pwex.map(count, filenames)\n",
    "print (\"working...\")\n",
    "pywren.wait(futures)\n",
    "\n",
    "result = sum([f.result() for f in futures])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should launch 73 PyWren tasks. After finishing the job, let's plot again to check the execution. This should look more interesting than the simple job before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from training import plot_pywren_execution\n",
    "plot_pywren_execution(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visits for English Pages\n",
    "Now let’s calculate the total view count on all English pages for each date in our dataset. From each record we want to extract three pieces of information: the access date, the view count, and language. \n",
    "Recall from above that the second field of a record is the `project code` For example, the project code `en` indicates an English page.\n",
    "\n",
    "With PyWren, we can `map` over every file with a function that will parse through the records in the file and return the sum of the view counts of each English page. After the jobs return, we can simply aggregate the page count sum for each file and add them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from functools import reduce\n",
    "\n",
    "def aggregate_count(key_value_list):\n",
    "    \n",
    "    def reduce_f(obj1, obj2):\n",
    "        return(obj1[0], obj1[1] + obj2[1])\n",
    "    \n",
    "    counts = [reduce(reduce_f, group) for _, group \n",
    "          in groupby(sorted(key_value_list), key=itemgetter(0))]\n",
    "    return counts\n",
    "\n",
    "def english_page_count(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    \n",
    "    # return True if a record corresponds to an english page.\n",
    "    def is_page_english(record):\n",
    "        # Fix me.\n",
    "        return False\n",
    "    \n",
    "    # filter out the english pages\n",
    "    en_pages = filter(is_page_english, data.split(\"\\n\"))\n",
    "    \n",
    "    # projection to create (date, pagecount) pairs\n",
    "    def make_date_viewcount_pair(record):\n",
    "        split_record = record.split()\n",
    "        \n",
    "        # the daate is the first entry of the record.\n",
    "        # We only want the YYYYMMDD characters.\n",
    "        date = split_record[0][:8]\n",
    "        \n",
    "        view_count = int(split_record[3])\n",
    "        return (date, view_count)\n",
    "    \n",
    "    en_kvpair_list = [make_date_viewcount_pair(p) for p in en_pages]\n",
    "\n",
    "    return aggregate_count(en_kvpair_list)\n",
    "    \n",
    "futures = pwex.map(english_page_count, filenames)\n",
    "pywren.wait(futures)\n",
    "\n",
    "results = [f.result() for f in futures]\n",
    "en_page_counts_by_date = aggregate_count([x for y in results for x in y])\n",
    "print(en_page_counts_by_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bonus: Finding the 15 most popular pages.\n",
    "Suppose we wanted to find the 15 most popular pages in the dataset. One immediate, \"naive\" solution would be to `map` a function that takes in a filename and returns the `(page_views, title)` pair for each record in the file, and then find the 15 most viewed pages from the returned results. This approach however, involves us iterating through some 51 million records, when we ideally could do some more work in parallel.\n",
    "\n",
    "To offload more work to the cloud, we could change our `map` function to return the 15 most popular records in each file. This would reduce the amount of work we do on our single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywren\n",
    "pwex = pywren.default_executor()\n",
    "\n",
    "# Takes a list of (size, page_title) pairs\n",
    "# and returns the 15 largest kv pairs.\n",
    "def fifteen_biggest(list_of_pairs):\n",
    "    # fill me in.\n",
    "    # Hint. You can use a min heap. https://docs.python.org/2/library/heapq.html\n",
    "    return []\n",
    "    \n",
    "def process_file(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    data = data.split(\"\\n\")\n",
    "    data = filter(lambda x: len(x) > 4, data)\n",
    "    \n",
    "    # Takes a single record and returns a tuple of (page_size, page_title)\n",
    "    # Page_size should be an int\n",
    "    def extract_kv_pair(record):\n",
    "        record = record.split(\" \")\n",
    "        # Fix me.\n",
    "        return (None, None)\n",
    "    data = map(extract_kv_pair, data)\n",
    "    return fifteen_biggest(data)\n",
    "\n",
    "futures = pwex.map(process_file, filenames)\n",
    "results = pwex.get_all_results(futures)\n",
    "\n",
    "flattened_list = []\n",
    "for result in results:\n",
    "    for elem in result:\n",
    "        flattened_list.append(elem)\n",
    "\n",
    "final_list = fifteen_biggest(flattened_list)\n",
    "for item in final_list:\n",
    "    print(item)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

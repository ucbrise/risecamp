{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyWren RISECamp, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics with PyWren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use PyWren explore a dataset of Wikipedia records.\n",
    "\n",
    "## 0. The Data\n",
    "\n",
    "We've prepared an S3 bucket with 20GB of Wikipedia traffic statistics data obtained from http://aws.amazon.com/datasets/4182. To make the analysis more feasible for the short time you're here, we've shortened the dataset to three days worth of data (May 5 to May 7, 2009; roughly 20G and 329 million entries). \n",
    "\n",
    "Let's take a look into the bucket with our dataset. We'll print a few files from a few files from our bucket.\n",
    "\n",
    "***Execute*** the code below to print out the names of the first 20 files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines are only needed for the solutions.\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# some libraries that are useful for this tutorial\n",
    "from training import wikipedia_bucket, list_keys_with_prefix, read_from_s3\n",
    "\n",
    "filenames = list_keys_with_prefix(wikipedia_bucket, \"wikistats_20090505_restricted-01/\")\n",
    "for filename in filenames[:20]:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 74 files (2 of which are intentionally left empty). Each file consists of a list of records. Let's go take a look into the first file. This may take a while.\n",
    "\n",
    "***Execute*** the code below to print out 20 records from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file into memory, and split on newlines.\n",
    "records = read_from_s3(wikipedia_bucket, \"wikistats_20090505_restricted-01/part-00001\").split(\"\\n\")\n",
    "\n",
    "print(\"The total number of records in this file is {}, but here are the first 20\".format(len(records)))\n",
    "for i in range(20):\n",
    "    print(records[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record has stats for a single Wikipedia page. The schema is:\n",
    "\n",
    "`<date_time> <project_code> <page_name> <page_views> <page_size>`\n",
    "\n",
    "- `<date_time>` specifies a date in YYYYMMDD-HHmmSS format (year, month, day, hour minute, second).\n",
    "- `<project_code>` specifies the language the page is written in.\n",
    "- `<page_title>` gives the page title.\n",
    "- `<page_views>` gives the number of page views in the hour-long time slot starting at `<data_time>`. \n",
    "- `<page_size>` gives the size in bytes of the  page.\n",
    "\n",
    "Now that we have a better understanding of the structure of our data, we can start running some interesting queries. \n",
    "\n",
    "Because the data are so large, it takes us quite a while to load just one file, as we saw in the previous exercise. You could imagine it would take way too long to process all of the data sequentially. Thankfully, we have [PyWren](https://www.youtube.com/watch?v=VSaDPc1Cs5U)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. To the ~~Batmobile~~ Cloud!!!!!!\n",
    "\n",
    "***Execute*** the code below to initialize an executor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to load PyWren and create an executor instance\n",
    "import pywren\n",
    "pwex = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Count\n",
    "Let’s see how many records in total are in this data set.\n",
    "\n",
    "***Exercise***: modify `count` to return the number of records in a given file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    return (len(data.split(\"\\n\")) if data else 0)    \n",
    "\n",
    "print (\"invoking pywren jobs...\")\n",
    "futures = pwex.map(count, filenames)\n",
    "print (\"working...\")\n",
    "pywren.wait(futures)\n",
    "\n",
    "result = sum([f.result() for f in futures])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should launch 73 PyWren tasks. After finishing the job, let's plot again to check the execution. This should look more interesting than the simple job before.\n",
    "\n",
    "***Execute*** the code below to show a plot of the executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from training import plot_pywren_execution\n",
    "plot_pywren_execution(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visits for English Pages\n",
    "Recall from above that the second field of a record is the `project code` For example, the project code `en` indicates an English page. Let’s calculate the view count on English pages for each date in our dataset.\n",
    "\n",
    "***Exercise:*** modify `is_page_english` so that it return true if a given record corresponds to an English page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from functools import reduce\n",
    "\n",
    "def aggregate_count(key_value_list):\n",
    "    \n",
    "    def reduce_f(obj1, obj2):\n",
    "        return(obj1[0], obj1[1] + obj2[1])\n",
    "    \n",
    "    counts = [reduce(reduce_f, group) for _, group \n",
    "          in groupby(sorted(key_value_list), key=itemgetter(0))]\n",
    "    return counts\n",
    "\n",
    "def english_page_count(filename):\n",
    "    data = read_from_s3(wikipedia_bucket, filename)\n",
    "    \n",
    "    # return True if a record corresponds to an english page.\n",
    "    def is_page_english(page):\n",
    "        if len(page.split(\" \")) >= 4 and page.split(\" \")[1] == \"en\":\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # filter out the english pages\n",
    "    en_pages = filter(is_page_english, data.split(\"\\n\"))\n",
    "    \n",
    "    # projection to create (date, pagecount) pairs\n",
    "    def make_date_viewcount_pair(record):\n",
    "        split_record = record.split()\n",
    "        \n",
    "        # the daate is the first entry of the record.\n",
    "        # We only want the YYYYMMDD characters.\n",
    "        date = split_record[0][:8]\n",
    "        \n",
    "        view_count = int(split_record[3])\n",
    "        return (date, view_count)\n",
    "    \n",
    "    en_kvpair_list = [make_date_viewcount_pair(p) for p in en_pages]\n",
    "\n",
    "    return aggregate_count(en_kvpair_list)\n",
    "    \n",
    "futures = pwex.map(english_page_count, filenames)\n",
    "pywren.wait(futures)\n",
    "\n",
    "results = [f.result() for f in futures]\n",
    "en_page_counts_by_date = aggregate_count([x for y in results for x in y])\n",
    "print(en_page_counts_by_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
